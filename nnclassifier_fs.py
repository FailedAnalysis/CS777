# -*- coding: utf-8 -*-
"""classifier_41.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19eqOkBcE9yFjErM2NPD7sAY9kaerI8-j
"""

import argparse
import os
import json
import numpy as np
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql import DataFrame
from pyspark.ml.linalg import Vectors, VectorUDT

DEFAULT_TEST_SPLIT_RATIO = 0.2
DEFAULT_RANDOM_STATE = 321
LABEL_COLUMN = 'label'
CELL_COLUMN = 'cell'
PARTITION_FACTOR = 2
DEFAULT_NUM_PARTITIONS = 200

LEARNING_RATE = 1e-3
NUM_EPOCHS = 10

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def softmax(x):
    exp_x = np.exp(x - np.max(x))
    return exp_x / exp_x.sum(axis=0)

def preprocess_data(spark: SparkSession, input_paths: list, num_partitions: int) -> tuple[DataFrame, DataFrame, list]:
    print("Starting data preprocessing...")

    df = spark.read.csv(input_paths, header=True, inferSchema=True)

    print(f"Initial dataframe loaded with {df.count()} rows and {len(df.columns)} columns.")
    print(f"Repartitioning to {num_partitions} partitions...")
    df = df.repartition(num_partitions)
    print(f"DataFrame repartitioned to {df.rdd.getNumPartitions()} partitions.")

    for col_name in (LABEL_COLUMN, CELL_COLUMN):
        if col_name not in df.columns:
            print(f"Error: Required column '{col_name}' not found in input data.")
            raise ValueError(f"Required column '{col_name}' not found.")

    feature_cols = [c for c in df.columns if c not in (LABEL_COLUMN, CELL_COLUMN)]
    print(f"Identified {len(feature_cols)} feature columns.")

    print(f"Indexing label column '{LABEL_COLUMN}'...")
    idx_model = StringIndexer(inputCol=LABEL_COLUMN, outputCol="label_indexed").fit(df)
    df = idx_model.transform(df)
    label_classes = idx_model.labels
    print(f"Indexed labels: {label_classes}")

    print("Assembling feature vector...")
    df = VectorAssembler(inputCols=feature_cols, outputCol="features_vector").transform(df)

    print("Scaling features...")
    scaler_model = MinMaxScaler(inputCol="features_vector", outputCol="features_scaled").fit(df)
    df = scaler_model.transform(df)

    processed_rdd = df.select("features_scaled", "label_indexed").rdd.map(
        lambda row: (row["features_scaled"].toArray(), row["label_indexed"])
    )

    print(f"Splitting data into train ({1 - DEFAULT_TEST_SPLIT_RATIO:.2f}) and test ({DEFAULT_TEST_SPLIT_RATIO:.2f}) sets...")
    train_rdd, test_rdd = processed_rdd.randomSplit(
        [1 - DEFAULT_TEST_SPLIT_RATIO, DEFAULT_TEST_SPLIT_RATIO],
        seed=DEFAULT_RANDOM_STATE
    )
    print(f"Train set size: {train_rdd.count()} rows")
    print(f"Test set size: {test_rdd.count()} rows")

    print("Preprocessing complete. Returning train and test RDDs and label classes.")
    return train_rdd, test_rdd, label_classes


def initialize_weights(input_size, hidden_layer1_size, hidden_layer2_size, output_size):
    np.random.seed(DEFAULT_RANDOM_STATE)

    W1 = np.random.randn(hidden_layer1_size, input_size) * 0.01
    b1 = np.zeros((hidden_layer1_size, 1))

    W2 = np.random.randn(hidden_layer2_size, hidden_layer1_size) * 0.01
    b2 = np.zeros((hidden_layer2_size, 1))

    W3 = np.random.randn(output_size, hidden_layer2_size) * 0.01
    b3 = np.zeros((output_size, 1))

    return {"W1": W1, "b1": b1, "W2": W2, "b2": b2, "W3": W3, "b3": b3}

def forward_backward_pass(partition_data, weights, num_classes):
    grad_W1 = np.zeros_like(weights["W1"])
    grad_b1 = np.zeros_like(weights["b1"])
    grad_W2 = np.zeros_like(weights["W2"])
    grad_b2 = np.zeros_like(weights["b2"])
    grad_W3 = np.zeros_like(weights["W3"])
    grad_b3 = np.zeros_like(weights["b3"])

    for features, label_indexed in partition_data:
        X = features.reshape(-1, 1)

        Z1 = np.dot(weights["W1"], X) + weights["b1"]
        A1 = sigmoid(Z1)

        Z2 = np.dot(weights["W2"], A1) + weights["b2"]
        A2 = sigmoid(Z2)

        Z3 = np.dot(weights["W3"], A2) + weights["b3"]
        A3 = softmax(Z3)

        Y = np.zeros((num_classes, 1))
        Y[int(label_indexed), 0] = 1

        dZ3 = A3 - Y

        grad_W3 += np.dot(dZ3, A2.T)
        grad_b3 += dZ3

        dA2 = np.dot(weights["W3"].T, dZ3)
        dZ2 = dA2 * sigmoid_derivative(A2)

        grad_W2 += np.dot(dZ2, A1.T)
        grad_b2 += dZ2

        dA1 = np.dot(weights["W2"].T, dZ2)
        dZ1 = dA1 * sigmoid_derivative(A1)

        grad_W1 += np.dot(dZ1, X.T)
        grad_b1 += dZ1

    num_samples_in_partition = len(list(partition_data))
    if num_samples_in_partition > 0:
         grad_W1 /= num_samples_in_partition
         grad_b1 /= num_samples_in_partition
         grad_W2 /= num_samples_in_partition
         grad_b2 /= num_samples_in_partition
         grad_W3 /= num_samples_in_partition
         grad_b3 /= num_samples_in_partition


    return {
        "grad_W1": grad_W1, "grad_b1": grad_b1,
        "grad_W2": grad_W2, "grad_b2": grad_b2,
        "grad_W3": grad_W3, "grad_b3": grad_b3
    }

def aggregate_gradients(grad1, grad2):
    return {
        "grad_W1": grad1["grad_W1"] + grad2["grad_W1"],
        "grad_b1": grad1["grad_b1"] + grad2["grad_b1"],
        "grad_W2": grad1["grad_W2"] + grad2["grad_W2"],
        "grad_b2": grad1["grad_b2"] + grad2["grad_b2"],
        "grad_W3": grad1["grad_W3"] + grad2["grad_W3"],
        "grad_b3": grad1["grad_b3"] + grad2["grad_b3"]
    }


def train_custom_mlp(spark: SparkSession, train_rdd, test_rdd, evaluation_output_path: str, label_classes: list):
    print("\nStarting custom MLP training...")

    try:
        sample_features, _ = train_rdd.take(1)[0]
        input_size = len(sample_features)
    except IndexError:
        print("Error: Training RDD is empty. Cannot determine input size.")
        return

    num_classes = len(label_classes)
    print(f"Input feature size: {input_size}")
    print(f"Number of classes: {num_classes}")

    hidden_layer1_size = 128
    hidden_layer2_size = 64
    print(f"MLP layers defined as: Input({input_size}) -> Hidden1({hidden_layer1_size}) -> Hidden2({hidden_layer2_size}) -> Output({num_classes})")


    weights = initialize_weights(input_size, hidden_layer1_size, hidden_layer2_size, num_classes)
    print("Model weights initialized.")

    print(f"Starting training for {NUM_EPOCHS} epochs...")
    for epoch in range(NUM_EPOCHS):
        print(f"Epoch {epoch + 1}/{NUM_EPOCHS}")

        broadcast_weights = spark.sparkContext.broadcast(weights)

        gradient_rdd = train_rdd.mapPartitions(
            lambda partition_data: [forward_backward_pass(partition_data, broadcast_weights.value, num_classes)]
        )

        zero_gradients = initialize_weights(input_size, hidden_layer1_size, hidden_layer2_size, num_classes)
        zero_gradients = {k: np.zeros_like(v) for k, v in zero_gradients.items()}

        def seqOp(acc, partition_grad):
            for key in acc:
                acc[key] += partition_grad[key]
            return acc

        def combOp(acc1, acc2):
            for key in acc1:
                acc1[key] += acc2[key]
            return acc1

        aggregated_gradients = gradient_rdd.treeAggregate(
            zero_gradients,
            seqOp,
            combOp
        )

        total_train_samples = train_rdd.count()
        if total_train_samples > 0:
             for key in aggregated_gradients:
                 aggregated_gradients[key] /= total_train_samples


        weights["W1"] -= LEARNING_RATE * aggregated_gradients["grad_W1"]
        weights["b1"] -= LEARNING_RATE * aggregated_gradients["grad_b1"]
        weights["W2"] -= LEARNING_RATE * aggregated_gradients["grad_W2"]
        weights["b2"] -= LEARNING_RATE * aggregated_gradients["grad_b2"]
        weights["W3"] -= LEARNING_RATE * aggregated_gradients["grad_W3"]
        weights["b3"] -= LEARNING_RATE * aggregated_gradients["grad_b3"]

        print(f"Epoch {epoch + 1} complete. Weights updated.")

    print("\nCustom MLP training complete.")

    print("Making predictions on the test set using custom model...")
    broadcast_final_weights = spark.sparkContext.broadcast(weights)

    def predict_sample(features, weights):
        X = features.reshape(-1, 1)

        Z1 = np.dot(weights["W1"], X) + weights["b1"]
        A1 = sigmoid(Z1)

        Z2 = np.dot(weights["W2"], A1) + weights["b2"]
        A2 = sigmoid(Z2)

        Z3 = np.dot(weights["W3"], A2) + weights["b3"]
        A3 = softmax(Z3)

        predicted_index = np.argmax(A3)
        return int(predicted_index)

    predictions_rdd = test_rdd.map(
        lambda row: (float(predict_sample(row[0], broadcast_final_weights.value)), float(row[1]))
    )

    from pyspark.sql.types import StructType, StructField, DoubleType
    predictions_schema = StructType([
        StructField("prediction", DoubleType(), False),
        StructField("label_indexed", DoubleType(), False)
    ])

    predictions_df = spark.createDataFrame(predictions_rdd, schema=predictions_schema)

    print("Evaluating model using MLlib Evaluator...")
    evaluator = MulticlassClassificationEvaluator(
        labelCol="label_indexed",
        predictionCol="prediction"
    )

    accuracy = evaluator.evaluate(predictions_df, {evaluator.metricName: "accuracy"})
    f1 = evaluator.evaluate(predictions_df, {evaluator.metricName: "f1"})
    weightedPrecision = evaluator.evaluate(predictions_df, {evaluator.metricName: "weightedPrecision"})
    weightedRecall = evaluator.evaluate(predictions_df, {evaluator.metricName: "weightedRecall"})

    print(f"Test Accuracy = {accuracy:.4f}")
    print(f"Test F1 Score = {f1:.4f}")
    print(f"Test Weighted Precision = {weightedPrecision:.4f}")
    print(f"Test Weighted Recall = {weightedRecall:.4f}")

    print(f"Saving evaluation results to {evaluation_output_path}...")
    try:
        results_data = [
            (f"Test Accuracy: {accuracy:.4f}",),
            (f"Test F1 Score: {f1:.4f}",),
            (f"Test Weighted Precision: {weightedPrecision:.4f}",),
            (f"Test Weighted Recall: {weightedRecall:.4f}",)
        ]
        results_df = spark.createDataFrame(results_data, ["result"])

        results_df.write.mode("overwrite").text(evaluation_output_path)

        print("Evaluation results saved successfully using Spark.")
    except Exception as e:
        print(f"Error saving evaluation results to {evaluation_output_path} using Spark: {e}")
        import traceback
        traceback.print_exc()

    print("Custom MLP training and evaluation complete.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Gene Expression Data Processing and Custom MLP Training Pipeline (Single File Input)")
    parser.add_argument(
        "input_file",
        nargs=1,
        help="Path to the single input CSV data file."
    )
    parser.add_argument(
        "evaluation_output_path",
        help="Path to save the model evaluation results (e.g., accuracy). This should be a directory path for Spark write."
    )
    parser.add_argument(
        "--num_partitions",
        type=int,
        default=DEFAULT_NUM_PARTITIONS,
        help=f"Desired number of partitions after initial data loading (default: {DEFAULT_NUM_PARTITIONS})."
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=LEARNING_RATE,
        help=f"Learning rate for Gradient Descent (default: {LEARNING_RATE})."
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=NUM_EPOCHS,
        help=f"Number of training epochs (default: {NUM_EPOCHS})."
    )

    args = parser.parse_args()

    LEARNING_RATE = args.learning_rate
    NUM_EPOCHS = args.num_epochs


    spark = SparkSession.builder.appName("GeneExpressionCustomMLPPipeline").getOrCreate()
    print("Spark session created.")

    try:
        train_rdd, test_rdd, label_classes = preprocess_data(spark, args.input_file, args.num_partitions)

        train_custom_mlp(spark, train_rdd, test_rdd, args.evaluation_output_path, label_classes)

    except Exception as e:
        print(f"An error occurred during the pipeline execution: {e}")
        import traceback
        traceback.print_exc()

    finally:
        spark.stop()
        print("Spark session stopped.")